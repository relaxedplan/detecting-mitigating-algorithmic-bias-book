{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    " \n",
    "#### Objective:\n",
    "\n",
    " - In the scenario of this LiveProject, you are tasked by the WHO to undertake an additional training program before conducting bias audits. This training program has three main objectives:\n",
    "  - The training program should guide you through the AIF360 demo, which will be useful for detecting and mitigating bias in Milestone 3\n",
    "  - The training program should introduce you to certain key bias metrics, which will be useful for detecting and mitigating bias in Milestone 3 and Milestone 4\n",
    "  - The training program should provide you an understanding of the limitations of bias detection and mitigation algorithms, through examples of algorithmic bias which cannot be measured.\n",
    "\n",
    "#### Workflow:\n",
    "\n",
    " - Become comfortable with allocation harm and algorithmic bias metrics in a binary classification context by reviewing online resources, especially AIF-360\n",
    " - Become aware of bias and ethical issues which cannot be measured by the algorithms introduced in this LiveProject, or cannot be measured by algorithms at all\n",
    " \n",
    "#### Importance to Project:\n",
    " - This milestone introduces learners to key concepts in algorithmic bias. It scaffolds algorithms which are covered in detail in detail in Milestone 3 and Milestone 4.\n",
    " \n",
    "#### Resources:\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias (This lists some common types of bias which arise when developing machine learning models.)\n",
    "\n",
    "https://aif360.mybluemix.net/data (This demo of AIF360, a debiasing tool, shows the detection of unwanted biases, and demonstrates how data or model manipulation can improve fairness for certain groups.)\n",
    "\n",
    "https://dssg.github.io/aequitas/metrics.html (document summarizing relevant metrics for evaluating bias mathematically) \n",
    "\n",
    "https://www.toptal.com/artificial-intelligence/mitigating-ai-bias (reviews some historical cases of AI bias, discusses in brief some detection and mitigation tools, discusses in brief some legal issues)\n",
    "\n",
    "https://sites.google.com/view/fatecv-tutorial/schedule?authuser=0 (A 3-part seminar reviewing algorithmic bias in computer vision. This seminar is particularly important because it describes how historical and societal context can influence the design and impact of machine learning models, to the detriment of historically marginalized groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Allocation harm and AIF360 demo\n",
    "\n",
    "Within milestones 3 and 4, you will learn to use algorithmic tools to detect and mitigate unwanted bias in the context of binary classification problems. Many difficult problems in health can be naturally expressed or reduced to binary classification problems, such as:\n",
    "\n",
    " - Is a region likely to experience a COVID outbreak? (similar to Milestone 1)\n",
    "\n",
    " - Is the patient likely to have a heart attack or not? (similar to Milestone 3)\n",
    "\n",
    " - What dose of medication should a patient be given for blood clot prevention, taking their physiology into account? (similar to Milestone 4)\n",
    "\n",
    "The types of bias which have received the most attention and the most development involve unfair distribution of a model's errors or predictions across groups. This is usually described as allocation harm.\n",
    "\n",
    "The following resources should help you get comfortable with bias metrics to measure allocation harm in a binary classification context:\n",
    "\n",
    "1.1\n",
    " - Take the \"Fairness\" portion of the Google Machine Leaning Crash Course. You may skip the \"programming exercise\", since we will cover a similar exercise in more depth in the next milestone.\n",
    "\n",
    "1.2\n",
    "\n",
    " - Take the AIF360 demo, on each of the datasets at https://aif360.mybluemix.net/data.\n",
    "\n",
    "  - At the \"check\" phase, review the definition of each bias metric. For milestones 3 and 4, __Average Odds Difference__, __Disparate Impact__, and __Equal Opportunity Difference__ are key.\n",
    "  - Taking the Compas dataset as an example, note who will lose out if the unwanted bias is not mitigated, and the real-world impact that can have.\n",
    "  - Taking the German Credit Scoring dataset as an example, test each mitigation approach. Choose the mitigation approach that removes the most bias, and take notes on how it works.\n",
    "\n",
    "For your reference, the definitions of __Average Odds Difference__, __Equal Opportunity Difference__ and __Disparate Impact__ are included below. In the next milestone, you will learn to calculate these manually from confusion matrices and with the AIF-360 package.  \n",
    "\n",
    "__Average odds difference__\n",
    " - This is the average of difference in __false positive rates__ (also known as specificity) and __true positive rates__ (also known as sensitivity) between unprivileged and privileged groups. A value of 0 implies both groups have equal benefit, a value less than 0 implies higher benefit for the privileged group and a value greater than 0 implies higher benefit for the unprivileged group.\n",
    " \n",
    "__Equal opportunity difference__\n",
    " - This is the difference in __true positive rates__ (also known as specificity) between unprivileged and privileged groups. A value of 0 implies both groups have equal benefit, a value less than 0 implies higher benefit for the privileged group and a value greater than 0 implies higher benefit for the unprivileged group.\n",
    " \n",
    "__Disparate Impact__\n",
    " \n",
    " - This is the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group. The ideal value of this metric is 1.0. A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Bias and ethical issues not covered in Milestones 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "While the bias detection and mitigation algorithms and packages covered in Milestones 3 and 4 cover most of the available algorithms and some biases, the true scope of algorithmic bias is enormous - much larger than allocation harm. \n",
    "\n",
    " - For certain types of algorithmic biases, special algorithms for bias mitigation and detection have been invented which are not covered in Milestones 3 and 4. \n",
    "\n",
    " - Many types of algorithmic biases are undetectable and unfixable by algorithms, and require much deeper upstream organizational or societal level change to resolve. \n",
    "\n",
    "For these biases as well as the allocative biases covered in Milestone 3 and 4, an understanding of the impact of your model and your work within its social and geopolitical context is key. This kind of understanding develops when data scientists and organizations continuously listen to the people affected by your model, especially those negatively impacted. Part of the value of Milestone 1 is that it has armed you with tools which can empower you to communicate with others about the reasoning underlying the models you build. \n",
    "\n",
    "To give you a sense of the diversity of challenges algorithmic bias, we will cover three examples and then reflect on the implications for data science practice and the place of algorithms described in Milestones 3 and 4. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data scientists are so pre-occupied with whether or not they can do a task, they don't stop to think if they should.\n",
    "\n",
    "In 2018, Stanford researchers Kosinski and Yilun [released](https://www.theguardian.com/technology/2018/jul/07/artificial-intelligence-can-tell-your-sexuality-politics-surveillance-paul-lewis) a paper demonstrating a facial recognition model to detect if subjects were gay. [Several organizations](https://www.glaad.org/blog/glaad-and-hrc-call-stanford-university-responsible-media-debunk-dangerous-flawed-report) representing LGBTQ people denounced the authors, not only because of methodological flaws but because of the potential consequences for LGBTQ people under regimes could use. They argued that the task should not exist in the first place. \n",
    "\n",
    "An emerging best practice requires data scientists to ask [whether a task should exist in the first place, who creates it, who will deploy it on which population, who owns the data, and how is it used](https://www.nytimes.com/2019/11/19/technology/artificial-intelligence-bias.html). As more socially aware data science becomes mainstream, fewer harmful models should be released into the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Datasets perpetuate the biases of the society in which they are created. Models built on those datasets perpetuate the biases of the society which created the dataset.\n",
    "\n",
    "1.  Kosinski and Yilun's facial recognition model described above was criticized for having a [binary representation of sexuality](https://www.glaad.org/blog/glaad-and-hrc-call-stanford-university-responsible-media-debunk-dangerous-flawed-report) for males (heterosexual / homosexual), and erasing bisexuality. The researchers compiled their own dataset from online dating apps and the choice to compile a dataset with a binary representation of sexuality was a choice made, at least partially, by the authors.\n",
    "\n",
    "\n",
    "2.  In Milestone 3, you will learn to detect and mitigate unwanted bias with respect to gender. The gender column in that dataset has a binary (man/woman) representation. This representation will exclude patients with non-binary gender identities, and some patients with intersex variations. If the dataset cannot be changed, not only can we not check for allocative bias for patients with non-binary gender identities, but we also are designing a model which perpetuates the binary representation in the training data and the social context where the training data was created (this is called representative bias). \n",
    "\n",
    "\n",
    "3.  State-of-the-art natural language processing models are trained on enormous volumes of data pulled from the internet. These models pick up on unwanted [stereotypes](https://arxiv.org/abs/1904.04047) and [biases](https://arxiv.org/abs/1904.04047) of the underlying training data as easily as they pick up language generally. These state-of-the-art models underlie most natural language processing applications today. While [debiasing algorithms](https://arxiv.org/ftp/arxiv/papers/1906/1906.08976.pdf) exist for these problems, they are not always effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. AI Systems require enormous resource extraction to build and maintain. This resource extraction is often unethical. \n",
    "\n",
    "⦁  Writing in the context of Amazon Echo, Kate Crawford and Vladan Joler describe three central, extractive processes that are required to run a large-scale artificial intelligence system: material resources, human labor, and data. Their [11 part essay](https://anatomyof.ai/) attempts to describe the enormous socio-technical systems which underlie the Echo and is well worth reading. \n",
    "\n",
    "⦁  A key component of Amazon Echos, smartphones, and laptops is lithium batteries. Almost 30% of all lithium is mined in the salt flates of Atacama, Chile. The Indigenous people of Atacama have been fighting for decades against mining permits and also for appropriate support in exchange for the extraction of lithium.\n",
    "\n",
    "⦁  Smart-phones leverage lithium batteries to collect huge amounts of data and act as an endpoint for machine learning models. Writing about lithium from Atacama, Liam Young and Kate Davies [observe](https://anatomyof.ai/) that \"your smart-phone runs on the tears and breast milk of a volcano. This landscape is connected to everywhere on the planet via the phones in our pockets; linked to each of us by invisible threads of commerce, science, politics and power.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing Part 1 and Part 2 together\n",
    "\n",
    "The techniques demoed in Part 1, which you will learn in subsequent milestones, are useful for detecting and mitigating algorithmic bias in a group fairness context. A model which passes bias detection metrics may still be biased both because what is considered fairness is so contextual and because so many types of bias exist which cannot be captured by algorithms. But a model which fails detection metrics should at least spark a conversation about appropriate mitigation strategies, and here algorithms for mitigating algorithmic bias can be very useful.\n",
    "\n",
    "The examples of bias in Part 2 show how data science is intimately related to social and geopolitical contexts. They also show the importance of deliberately looking to the effects of ML models and engaging with people who could be affected by them. The model interpretability techniques you learned in Milestone 1 can be very useful for greater transparency and engagement with the users and subjects of machine learning models. This increased diversity of perspectives helps catch biases in ML models as soon as possible.\n",
    "\n",
    "Indeed, it is unlikely that Kosinski and Wang were designing their model with the oppression of LGBTQ people in mind, yet representative organizations immediately expressed concern about social and geopolitical implications. Racial and religious biases were not hard-coded into NLP models, but those models are a reflection of our society and geopolitics. Most data scientists developing ML models for mobile use, or training ML models from mobile data, are not thinking of the origins of the components of our ML-enabled smartphones.\n",
    "\n",
    "Ignoring social and geopolitical contexts when modelling is poor data science and increasing attention is being paid to them at the highest levels of ML research. Submissions to the NeurIPS conference must be accompanied by a [broader impact statement](https://medium.com/@GovAI/a-guide-to-writing-the-neurips-impact-statement-4293b723f832). Public agencies are releasing [AI impact statements](https://medium.com/@AINowInstitute/algorithmic-impact-assessments-toward-accountable-automation-in-public-agencies-bd9856e6fdde) in an effort to increase accountability and empower the public to respond. We should work towards leveraging these frameworks and questions in our own organizations to reduce the harm and increase the benefits of machine learning models alongside traditional performance metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
