{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 \n",
    "In this milestone our ultimate goal is to remove the disparate impact from a model. The steps are as follws:\n",
    " - Train a model with algorithmic gender bias\n",
    " - Understand the reasoning underlying the biased model\n",
    " - Measure the bias using disparate impact, thiel score, etc\n",
    " - Debias the model\n",
    " \n",
    "#### Objective:\n",
    "\n",
    " - Build a model with algorithmic gender bias and debias it \n",
    "\n",
    "#### Workflow:\n",
    "\n",
    " - Load data\n",
    " - Train a model without unwanted gender bias\n",
    " - Demonstrate using confusion matrices that the model is not significantly biased\n",
    " - Introduce bias into the data\n",
    " - Train a biased model\n",
    " - Demonstrate using confusion matrices that the model is significantly biased\n",
    " - Use SHAP explainer to explain the model's underlying predictions\n",
    " - Load the dataset into AIF360\n",
    " - Debias the model using a pre-processing algorithm and measure the remaining bias\n",
    " - Debias the model using an in-processing algorithm and measure the remaining bias\n",
    " - Debias the model using a post-processing algorithm and measure the remaining bias\n",
    " - Choose one of the approaches, recommend it for use, and explain why\n",
    "\n",
    "#### Importance to Project:\n",
    "\n",
    " - This milestone is the most challenging and important part of the project. It guides you through a simple model debiasing problem.\n",
    " \n",
    "#### Resources:\n",
    " - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6678298/\n",
    " - https://arxiv.org/pdf/1810.01943.pdf\n",
    " - https://aif360.mybluemix.net/check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import modules, register helper functions, and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using is a cardiology dataset described in the [Journal of Clinical Medicine](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6678298/). In the original dataset the 'cardio' column represented whether or not the patient has cardiovascular disease. \n",
    "    \n",
    "In our scenario, we can obtain all of the measurements in the dataset but we cannot detect cardiovascular disease. Furthermore, a patient which has a prediction of cardiovascular disease will be given a medication which reduces the likelihood of a cardiac event by 50%.\n",
    "\n",
    "To load the data, do the following:\n",
    " - import the relevant modules and helper functions\n",
    " - load the data from 'cardio_train.csv',\n",
    " - drop nan values\n",
    " - sort the data by 'cardio', the target variable\n",
    " - inspect the dataframe, and familiarize yourself with the labels (see table 2 of the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt    \n",
    "import numpy as np\n",
    "import shap\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "import aif360\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.explainers import MetricTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biased_dataset(dataset, women_yes_event_subsample_rate=0.3):\n",
    "    women_no_event = dataset[(dataset['gender']==1) & (dataset['cardio']==0)]\n",
    "    women_yes_event = dataset[(dataset['gender']==1) & (dataset['cardio']==1)]\n",
    "    men_no_event = dataset[(dataset['gender']==2) & (dataset['cardio']==0)]\n",
    "    men_yes_event = dataset[(dataset['gender']==2) & (dataset['cardio']==1)]\n",
    "\n",
    "    biased_dataset = pd.concat([\n",
    "        women_no_event,\n",
    "        women_yes_event.sample(int(len(women_yes_event)*women_yes_event_subsample_rate)),\n",
    "        men_no_event,\n",
    "        men_yes_event\n",
    "    ])\n",
    "    return biased_dataset\n",
    "\n",
    "def train_model(dataset, exclude_gender = False):\n",
    "    X_cols = [c for c in dataset.columns if c!='cardio']\n",
    "    X = dataset[X_cols]\n",
    "    y = dataset['cardio']\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    if exclude_gender:\n",
    "        X_cols.remove('gender')\n",
    "    rf.fit(X_train[X_cols], y_train)\n",
    "        \n",
    "    return rf, X_cols, X_train, X_test, y_train, y_test\n",
    "\n",
    " \n",
    "\n",
    "def plot_confusion_matrix(X,y, return_percentage=False):\n",
    "    \n",
    "    cm = confusion_matrix(X,y,labels=[1,0])\n",
    "    \n",
    "    if return_percentage:\n",
    "        cm = np.round(np.asarray(cm)/(np.asarray(cm).sum()),3)\n",
    "\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Greens',fmt='g'); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['positive', 'negative']); ax.yaxis.set_ticklabels(['positive', 'negative']);\n",
    "    plt.show()\n",
    "    \n",
    "def aif360_to_pandas(dataset_aif360):\n",
    "    return dataset.convert_to_dataframe()[0]\n",
    "def pandas_to_aif360(dataset_pd):\n",
    "    return aif360.datasets.BinaryLabelDataset(1,0,df=dataset_pd,label_names=['cardio'],protected_attribute_names=['gender'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train a model without gender bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model trained from this data will not be subject to significant bias. However, being able to recognize when a model isn't biased is an important part of the debiasing process. \n",
    "\n",
    " - Using the helper function train_model, train a model\n",
    " - Using the helper function plot_confusion_matrix, plot a confusion matrix based on the model's predictions for the test set (you may want to turn on 'return_percentage').\n",
    " - Split the patients in the test set by gender, and create a confusion matrix for each. \n",
    " - Confirm that the percentage allocations of the model are equal within a couple of percent. We will go into more detail about metrics later in the module, but in this case an eyeball test is all that is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaving this in so people can unders\n",
    "rf, X_cols,X_train, X_test, y_train, y_test = train_model(dataset)\n",
    "plot_confusion_matrix(y_test, rf.predict(X_test[X_cols]))\n",
    "plot_confusion_matrix(y_test, rf.predict(X_test[X_cols]),return_percentage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing gender bias into data \n",
    "\n",
    "We can introduce gender bias into the data by manipulating the ratio of positive to negative events for either gender. In this case, we will alter the dataset by removing 80% of the occurrences where women have cardiovascular disease. In other words, on the altered data, women will have a lower likelihood of cardiovascular disease.\n",
    "\n",
    " - Create the biased dataset using the generate_biased_dataset helper function (we recommend a women_yes_event_subsample_rate of 0.2)\n",
    " - Train a model based on this biased dataset using the train_model helper function\n",
    " - Plot a confusion matrix with the plot_confusion_matrix helper function\n",
    " - Create a confusion matrix for the subset of the test set where the gender is male, and a confusion matrix for the subset of the test set where gender is female. \n",
    " - Using the confusion matrix, or other techniques, look for bias: \n",
    "\n",
    "    -- the statistical parity differences between men and women (compare the ratio of predicted positive instances:negative instances) should be obvious.     \n",
    "    -- however, if you compare the predicted percentages of positive instances for women according to the model, with the predicted percentages of positive instances for women in the ground truth, you should see a substantial difference as well. This indicates an unwanted algorithmic bias, even accounting for the statistical parity differences we introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing gender bias in the model using SHAP\n",
    "\n",
    " - It is relatively simple to show that the model is considering gender by creating a SHAP summary plot, as we learnt in milestone 1. \n",
    " - If we were looking at this model for the first time, without prior knowledge that it had gender bias, seeing gender as a very important feature would be a red flag.\n",
    " - It is not possible to determine from the SHAP plot if the gender feature is given high importance because of genuine physiological differences, or if unwanted algorithmic bias is present.\n",
    "\n",
    "Generate a SHAP summary plot explaining the biased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why can't we just remove the 'gender' variable'?\n",
    "An interesting question: If the SHAP value is indicating that the 'gender' variable is important, and it seems to be a source of bias, could we simply remove it rather than going through a time-consuming debiasing process? \n",
    "\n",
    "Unfortunately, it is usually not so simple. In this instance, like many others, the model will be able to approximate gender using other variables. Removing the variable also reduces the accuracy of the model. As an optional exercise, if you are curious, you can train a model without the 'gender' variable and confirm that removing the gender variable does not resolve all the algorithmic bias issues. You can pass in __exclude_gender=True__ to the train_model helper function to achieve this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing AIF360 - Let's debias our biased model!\n",
    "\n",
    "At this stage, it's helpful to isolate exactly what kinds of bias we are interested in removing from our model. There are at least two obvious biases in the model we built:\n",
    " 1. Due directly to our tampering with the dataset, the model predicts more positive cases for men than women.\n",
    " 2. The model's errors are distributed unevenly across genders: women have a lower true positive rater than men.\n",
    "\n",
    "The question remains which of these biases are unwanted algorithmic biases, which we should remove. \n",
    "\n",
    "1. In this milestone, we artifically caused (1). However, disparities between men and women can be due to general physiological differences (for example, breast cancer is [70-100 times less common in men than in women](https://www.cancer.org/cancer/breast-cancer-in-men/about/key-statistics.html). With more knowledge about the experimental setup, a subject matter expert in cardiovascular disease would be able to give a qualified answer about the source of the bias and whether or not it's unwanted. For learning purposes, we will assume that this bias is unwanted and attempt to remove it.\n",
    "\n",
    "2. (2) is more straightforward. The model errors are unfairly distributed in such a way that borderline cases in women are more likely to be judged as negative than borderline cases in men. If we can resolve this without a substantial drop in overall accuracy, that would be preferable.\n",
    "\n",
    "For learning purposes, we will attempt to mitigate both types of bias. \n",
    "\n",
    "### Steps:\n",
    " - Load entire biased dataset into AIF360\n",
    " - Using ClassificationMetric, measure key bias metrics in the biased model's predictions\n",
    " - Create debiased models using each of the following: pre-processing debiasing method (reweighing), an in-processing debiasing method (prejudice remover), and a post-processing debiasing method (calibration).\n",
    " - Decide on a preferred debiasing approach and justify your choice\n",
    " \n",
    "For demonstration purposes, the below cell contains an audit of accuracy, statistical parity difference, and disparate impact, with respect to the model and the biased dataset. You can modify the code to report on relevant metrics for each debiasing method you choose to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "aif360_biased_dataset = pandas_to_aif360(biased_dataset) #this is a helper function, you could insetad use the below\n",
    "# aif360_biased_dataset = aif360.datasets.BinaryLabelDataset(1,0,df=biased_dataset,label_names=['cardio'],protected_attribute_names=['gender'])\n",
    "results = {}\n",
    "\n",
    "y_val_pred = rf.predict(aif360_biased_dataset.features)\n",
    "dataset_pred = aif360_biased_dataset.copy()\n",
    "dataset_pred.labels = y_val_pred\n",
    "dataset_pred.scores = rf.predict_proba(aif360_biased_dataset.features)[:,1]\n",
    "\n",
    "unprivileged_groups = [{'gender':2.0}] #women\n",
    "privileged_groups = [{'gender':1.0}] #men\n",
    "\n",
    "metric = ClassificationMetric(\n",
    "        aif360_biased_dataset, dataset_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "\n",
    "results['original_dataset'] = {'accuracy':metric.accuracy(),\n",
    "'statistical_parity_difference':metric.statistical_parity_difference(),\n",
    "'disparate_impact':metric.disparate_impact()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing debiasing (eg reweighing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-processing debiasing (eg Prejudice Remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing debiasing using Prejudice Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Of the debiasing methods you chose, recommend one as the best and justify your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
